' ==========================================================
' == A Simple Character-Level RNN in jdBasic
' == Goal: Train a model to predict the next character. 
' == This script has been updated to use the robust AI functions.
' ==========================================================

CLS
PRINT "--- Building a Character-Level RNN ---"
PRINT

' --- 1. Vocabulary and Data Setup ---
' The model learns to predict the next character from this text. 
TEXT_DATA$ = "hello world "

' Create a vocabulary of unique characters. 
VOCAB = []
FOR I = 0 TO LEN(TEXT_DATA$) - 1
    CHAR$ = MID$(TEXT_DATA$, I + 1, 1)
    IS_IN_VOCAB = 0
    IF LEN(VOCAB) > 0 THEN
        FOR J = 0 TO LEN(VOCAB) - 1
            IF VOCAB[J] = CHAR$ THEN
                IS_IN_VOCAB = 1
                EXITFOR ' EXITFOR exits the inner loop. 
            ENDIF
        NEXT J
    ENDIF
    IF IS_IN_VOCAB = 0 THEN
        VOCAB = APPEND(VOCAB, CHAR$)
    ENDIF
NEXT I

VOCAB_SIZE = LEN(VOCAB)

' Create a Map for easy character-to-index lookup. 
VOCAB_MAP = {}
FOR I = 0 TO VOCAB_SIZE - 1
    VOCAB_MAP{VOCAB[I]} = I
NEXT I

PRINT "Vocabulary:";
FOR I = 0 TO VOCAB_SIZE - 1
    PRINT " "; VOCAB[I];
NEXT I
PRINT
PRINT


' --- 2. Prepare Training Data ---
' The input is "hello world" and the target is "ello world ". 
INPUT_TEXT$ = LEFT$(TEXT_DATA$, LEN(TEXT_DATA$) - 1)
TARGET_TEXT$ = RIGHT$(TEXT_DATA$, LEN(TEXT_DATA$) - 1)

' Convert text to numerical tokens.
INPUT_TOKENS_ARRAY = TENSOR.TOKENIZE(INPUT_TEXT$, VOCAB_MAP)
TARGET_TOKENS_ARRAY = TENSOR.TOKENIZE(TARGET_TEXT$, VOCAB_MAP)

PRINT "Input tokens:"; FRMV$(INPUT_TOKENS_ARRAY) 
PRINT "Target tokens:"; FRMV$(TARGET_TOKENS_ARRAY)
PRINT


' --- 3. Model Definition ---
' We define the model as an array of layers.
HIDDEN_DIM = 64
EMBEDDING_DIM = HIDDEN_DIM

MODEL = []
' Layer 0: Embedding Layer (maps a character to a dense vector)
MODEL = APPEND(MODEL, TENSOR.CREATE_LAYER("EMBEDDING", {"vocab_size": VOCAB_SIZE, "embedding_dim": EMBEDDING_DIM}))

' Layer 1: First RNN Hidden Layer
MODEL = APPEND(MODEL, TENSOR.CREATE_LAYER("DENSE", {"input_size": HIDDEN_DIM, "units": HIDDEN_DIM}))

' Layer 2: Second RNN Hidden Layer (making the RNN "deeper")
MODEL = APPEND(MODEL, TENSOR.CREATE_LAYER("DENSE", {"input_size": HIDDEN_DIM, "units": HIDDEN_DIM}))

' Layer 3: Output Layer (maps the final hidden state back to character probabilities)
MODEL = APPEND(MODEL, TENSOR.CREATE_LAYER("DENSE", {"input_size": HIDDEN_DIM, "units": VOCAB_SIZE}))

OPTIMIZER = TENSOR.CREATE_OPTIMIZER("SGD", {"learning_rate": 0.05})
EPOCHS = 2500 ' The number of training iterations. 


' --- 4. Helper Functions ---

' Creates a one-hot encoded vector as a [1, size] matrix.
FUNC ONE_HOT_ENCODE(token_id, size)
    vec = []
    DIM vec[size]
    vec[token_id] = 1
    RETURN RESHAPE(vec, [1, size])
ENDFUNC

' Defines the forward pass for one time-step of the RNN.
FUNC FORWARD_PASS(input_token_tensor, prev_h1, prev_h2) 
    ' Embedding: Convert character index to a dense vector.
    one_hot = ONE_HOT_ENCODE(TENSOR.TOARRAY(input_token_tensor)[0], VOCAB_SIZE)
    embedded = TENSOR.MATMUL(TENSOR.FROM(one_hot), MODEL[0]{"weights"})

    ' --- RNN Cell 1 ---
    ' Combine current input with previous hidden state.
    combined_1 = embedded + prev_h1
    ' Calculate the new hidden state.
    new_h1 = TENSOR.RELU(TENSOR.MATMUL(combined_1, MODEL[1]{"weights"}) + MODEL[1]{"bias"})

    ' --- RNN Cell 2 ---
    combined_2 = new_h1 + prev_h2
    new_h2 = TENSOR.RELU(TENSOR.MATMUL(combined_2, MODEL[2]{"weights"}) + MODEL[2]{"bias"})

    ' --- Output Layer ---
    ' Predict the logits for the next character.
    logits = TENSOR.MATMUL(new_h2, MODEL[3]{"weights"}) + MODEL[3]{"bias"} 
    
    ' Return the prediction and the new hidden states for the next time-step.
    RETURN [logits, new_h1, new_h2]
ENDFUNC

' Chooses a character index based on the model's output probabilities.
FUNC SAMPLE(probs_array)
    r = RND(1)
    cdf = 0
    num_probs = LEN(probs_array)[1] ' Get the number of columns (vocab size)
    FOR i = 0 TO num_probs - 1
        cdf = cdf + probs_array[0, i]
        IF r < cdf THEN 
            RETURN i ' Return the selected index.
        ENDIF
    NEXT i
    RETURN num_probs - 1 ' Fallback
ENDFUNC


' --- 5. Training Loop ---
PRINT "--- Starting Training... ---"
PRINT

FOR EPI = 1 TO EPOCHS
    TOTAL_LOSS = 0
    
    ' Initialize hidden states to zero at the start of each sequence.
    DIM h1_array[HIDDEN_DIM] : h1 = TENSOR.FROM(RESHAPE(h1_array, [1, HIDDEN_DIM]))
    DIM h2_array[HIDDEN_DIM] : h2 = TENSOR.FROM(RESHAPE(h2_array, [1, HIDDEN_DIM]))

    ' Process the text one character at a time.
    FOR I = 0 TO LEN(INPUT_TOKENS_ARRAY) - 1
        input_tensor = TENSOR.FROM([INPUT_TOKENS_ARRAY[I]])
        target_tensor = TENSOR.FROM(ONE_HOT_ENCODE(TARGET_TOKENS_ARRAY[I], VOCAB_SIZE))

        ' --- Perform one step of the RNN ---
        forward_result = FORWARD_PASS(input_tensor, h1, h2)
        logits_tensor = forward_result[0]
        h1 = forward_result[1] ' Update hidden states for the next character. 
        h2 = forward_result[2]

        ' --- Calculate loss and update the model ---
        loss_tensor = TENSOR.CROSS_ENTROPY_LOSS(logits_tensor, target_tensor)
        TOTAL_LOSS = TOTAL_LOSS + TENSOR.TOARRAY(loss_tensor)[0]
        TENSOR.BACKWARD loss_tensor
        MODEL = TENSOR.UPDATE(MODEL, OPTIMIZER)
    NEXT I

    IF EPI MOD 100 = 0 THEN
        PRINT "Epoch:"; EPI; ", Average Loss:"; TOTAL_LOSS / LEN(INPUT_TOKENS_ARRAY)
    ENDIF
NEXT EPI

PRINT
PRINT "--- Training Complete ---"
PRINT


' --- 6. Inference / Text Generation ---
PRINT
PRINT "--- Generating Text ---"
PRINT "Seed character: 'h'"

generated_text$ = "h"
current_token_id = VOCAB_MAP{"h"}

' Initialize hidden states for generation.
DIM h1_gen_array[HIDDEN_DIM] : h1_gen = TENSOR.FROM(RESHAPE(h1_gen_array, [1, HIDDEN_DIM]))
DIM h2_gen_array[HIDDEN_DIM] : h2_gen = TENSOR.FROM(RESHAPE(h2_gen_array, [1, HIDDEN_DIM]))

FOR i = 1 TO 100 ' Generate 100 characters
    input_tensor = TENSOR.FROM([current_token_id])
    
    ' Pass the current character and hidden state to the model.
    inference_result = FORWARD_PASS(input_tensor, h1_gen, h2_gen)
    logits_tensor = inference_result[0]
    h1_gen = inference_result[1] ' Update the hidden state for the next generation step.
    h2_gen = inference_result[2]

    ' Convert logits to probabilities and sample the next character.
    probs_tensor = TENSOR.SOFTMAX(logits_tensor)
    probs_array = TENSOR.TOARRAY(probs_tensor)
    next_token_id = SAMPLE(probs_array)
    
    generated_text$ = generated_text$ + VOCAB[next_token_id]
    current_token_id = next_token_id
NEXT i

PRINT
PRINT "Generated Output:"
PRINT generated_text$
PRINT