' ==========================================================
' == A Simple Character-Level Language Model in jdBasic
' == Goal: Train a model to predict the next character in a sequence.
' ==========================================================

CLS
PRINT "--- Building a Deeper LLM (with Sampling) ---"
PRINT

' --- 1. Vocabulary and Data Setup ---
' This is our simple training text. The model will learn to predict the next character.
TEXT_DATA$ = "hello world "

' Create a vocabulary from the unique characters in the text data.
VOCAB = []
FOR I = 0 TO LEN(TEXT_DATA$) - 1
    CHAR$ = MID$(TEXT_DATA$, I + 1, 1)
    ' Check if character is already in our vocab
    IS_IN_VOCAB = 0
    if LEN(VOCAB) > 0 then
        FOR J = 0 TO LEN(VOCAB) - 1
            IF VOCAB[J] = CHAR$ THEN
                IS_IN_VOCAB = 1
                EXITFOR
            ENDIF
        NEXT J
    Endif
    
    IF IS_IN_VOCAB = 0 THEN
        VOCAB = APPEND(VOCAB, CHAR$)
    ENDIF
NEXT I

VOCAB_SIZE = LEN(VOCAB)

' Create a Map for easy character-to-index lookup.
VOCAB_MAP = {}
FOR I = 0 TO VOCAB_SIZE - 1
    VOCAB_MAP{VOCAB[I]} = I
NEXT I

PRINT "Vocabulary ("; VOCAB_SIZE; " unique chars): ";
FOR I = 0 TO VOCAB_SIZE - 1
    PRINT VOCAB[I];
NEXT I
PRINT
PRINT


' --- 2. Prepare Training Data ---
' Input is the text, Target is the text shifted by one character.
INPUT_TEXT$ = LEFT$(TEXT_DATA$, LEN(TEXT_DATA$) - 1)
TARGET_TEXT$ = RIGHT$(TEXT_DATA$, LEN(TEXT_DATA$) - 1)

' Tokenize the text into numerical arrays
INPUT_TOKENS_ARRAY = TENSOR.TOKENIZE(INPUT_TEXT$, VOCAB_MAP)
TARGET_TOKENS_ARRAY = TENSOR.TOKENIZE(TARGET_TEXT$, VOCAB_MAP)

PRINT "Input tokens:  "; FRMV$(INPUT_TOKENS_ARRAY)
PRINT "Target tokens: "; FRMV$(TARGET_TOKENS_ARRAY)
PRINT


' --- 3. Model Definition ---
HIDDEN_DIM = 64     ' Increased hidden layer size for more capacity
EMBEDDING_DIM = HIDDEN_DIM

' *** MODIFIED: Initialize MODEL as a Map {} instead of an Array [] ***
MODEL = {}

' Layer 0: Embedding
' *** MODIFIED: Break chained assignment into two steps to avoid parser issues ***
embedding_layer = TENSOR.CREATE_LAYER("EMBEDDING", {"vocab_size": VOCAB_SIZE, "embedding_dim": EMBEDDING_DIM})
DUMMY_BIAS_ARRAY = [] : DIM DUMMY_BIAS_ARRAY[EMBEDDING_DIM]
embedding_layer{"bias"} = TENSOR.FROM(RESHAPE(DUMMY_BIAS_ARRAY, [1, EMBEDDING_DIM])) 
MODEL{"embedding"} = embedding_layer

' Layer 1: First Hidden RNN Layer
MODEL{"dense1"} = TENSOR.CREATE_LAYER("DENSE", {"input_size": HIDDEN_DIM, "units": HIDDEN_DIM})

' Layer 2: Second Hidden RNN Layer
MODEL{"dense2"} = TENSOR.CREATE_LAYER("DENSE", {"input_size": HIDDEN_DIM, "units": HIDDEN_DIM})

' Layer 3: Final Output Layer
MODEL{"output"} = TENSOR.CREATE_LAYER("DENSE", {"input_size": HIDDEN_DIM, "units": VOCAB_SIZE})

OPTIMIZER = TENSOR.CREATE_OPTIMIZER("SGD", {"learning_rate": 0.05})
EPOCHS = 2500 ' Increased epochs for deeper model


' --- 4. Helper Functions ---

' Helper to create a one-hot encoded 2D matrix [1, size]
FUNC ONE_HOT_ENCODE(token_id, size)
    vec = []
    DIM vec[size] ' This initializes with 0.0 which is faster
    vec[token_id] = 1
    ' Ensure the vector is a 2D matrix
    RETURN RESHAPE(vec, [1, size])
ENDFUNC

' *** MODIFIED: The forward pass now accepts the model map as a parameter ***
FUNC FORWARD_PASS(current_model, input_token_tensor, prev_h1, prev_h2)
    ' Embedding Lookup
    input_index = TENSOR.TOARRAY(input_token_tensor)[0]
    one_hot_matrix_array = ONE_HOT_ENCODE(input_index, VOCAB_SIZE)
    one_hot_matrix_tensor = TENSOR.FROM(one_hot_matrix_array)
    embedded_tensor = TENSOR.MATMUL(one_hot_matrix_tensor, current_model{"embedding"}{"weights"})

    ' --- Layer 1 ---
    combined_input_1 = embedded_tensor + prev_h1
    new_h1 = TENSOR.RELU(TENSOR.MATMUL(combined_input_1, current_model{"dense1"}{"weights"}) + current_model{"dense1"}{"bias"})

    ' --- Layer 2 ---
    combined_input_2 = new_h1 + prev_h2
    new_h2 = TENSOR.RELU(TENSOR.MATMUL(combined_input_2, current_model{"dense2"}{"weights"}) + current_model{"dense2"}{"bias"})

    ' --- Output Layer ---
    logits = TENSOR.MATMUL(new_h2, current_model{"output"}{"weights"}) + current_model{"output"}{"bias"}
    
    ' Return logits and the new hidden states
    RETURN [logits, new_h1, new_h2]
ENDFUNC


' --- NEW: Function to sample from a probability distribution ---
FUNC SAMPLE(probs_array)
    r = RND(1) ' Get a random number between 0 and 1
    cdf = 0 ' Cumulative distribution function
    num_probs = LEN(probs_array)[1]

    FOR i = 0 TO num_probs - 1
        cdf = cdf + probs_array[0, i]
        IF r < cdf THEN
            RETURN i ' Return the index of the chosen character
        ENDIF
    NEXT i
    RETURN num_probs - 1 ' Fallback in case of floating point inaccuracies
ENDFUNC


' --- 5. Training Loop ---
PRINT "--- Starting Training... ---"
PRINT

FOR EPI = 1 TO EPOCHS
    TOTAL_LOSS = 0
    
    ' Initialize hidden states
    DIM h1_array[HIDDEN_DIM] : h1 = TENSOR.FROM(RESHAPE(h1_array, [1, HIDDEN_DIM]))
    DIM h2_array[HIDDEN_DIM] : h2 = TENSOR.FROM(RESHAPE(h2_array, [1, HIDDEN_DIM]))

    FOR I = 0 TO LEN(INPUT_TOKENS_ARRAY) - 1
        input_token_id = INPUT_TOKENS_ARRAY[I]
        target_token_id = TARGET_TOKENS_ARRAY[I]
        input_tensor = TENSOR.FROM([input_token_id])
        target_tensor = TENSOR.FROM(ONE_HOT_ENCODE(target_token_id, VOCAB_SIZE))

        ' --- FORWARD PASS ---
        ' *** MODIFIED: Pass the MODEL map to the function ***
        forward_result = FORWARD_PASS(MODEL, input_tensor, h1, h2)
        logits_tensor = forward_result[0]
        h1 = forward_result[1] ' Carry over the state
        h2 = forward_result[2]

        ' --- CALCULATE LOSS, BACKPROPAGATE, AND UPDATE ---
        loss_tensor = TENSOR.CROSS_ENTROPY_LOSS(logits_tensor, target_tensor)
        TOTAL_LOSS = TOTAL_LOSS + TENSOR.TOARRAY(loss_tensor)[0]
        
        TENSOR.BACKWARD loss_tensor
        MODEL = TENSOR.UPDATE(MODEL, OPTIMIZER)
    NEXT I

    IF EPI MOD 100 = 0 THEN
        PRINT "Epoch:"; EPI; ", Average Loss:"; TOTAL_LOSS / LEN(INPUT_TOKENS_ARRAY)
    ENDIF
NEXT EPI

PRINT
PRINT "--- Training Complete ---"
PRINT

' --- 6. Inference / Text Generation ---
PRINT "--- Generating Text ---"
PRINT "Seed character: 'h'"

generated_text$ = "h"
current_token_id = VOCAB_MAP{"h"}

' Initialize hidden states for generation
DIM h1_gen_array[HIDDEN_DIM] : h1_gen = TENSOR.FROM(RESHAPE(h1_gen_array, [1, HIDDEN_DIM]))
DIM h2_gen_array[HIDDEN_DIM] : h2_gen = TENSOR.FROM(RESHAPE(h2_gen_array, [1, HIDDEN_DIM]))

FOR i = 1 TO 100 ' Generate 100 characters
    input_tensor = TENSOR.FROM([current_token_id])
    
    ' Get logits and the next hidden states
    ' *** MODIFIED: Pass the MODEL map to the function ***
    inference_result = FORWARD_PASS(MODEL, input_tensor, h1_gen, h2_gen)
    logits_tensor = inference_result[0]
    h1_gen = inference_result[1]
    h2_gen = inference_result[2]

    predicted_probs_tensor = TENSOR.SOFTMAX(logits_tensor)
    probs_2d_array = TENSOR.TOARRAY(predicted_probs_tensor)

    ' --- NEW: Use sampling instead of just the max probability ---
    next_token_id = SAMPLE(probs_2d_array)
    
    generated_text$ = generated_text$ + VOCAB[next_token_id]
    current_token_id = next_token_id
NEXT i

PRINT
PRINT "Generated Output:"
PRINT generated_text$
PRINT
